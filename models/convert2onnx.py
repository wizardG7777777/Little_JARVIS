import os
import yaml
import torch
import logging
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import sys
from datetime import datetime

# æ ¸å¿ƒåº“å¯¼å…¥
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model, TaskType
import onnx
import onnxruntime as ort
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer
from optimum.onnxruntime.configuration import OptimizationConfig
from datasets import Dataset
import numpy as np
import torch.nn.utils.prune as prune

# è®¾ç½®æ—¥å¿— - ä¿®å¤Unicodeç¼–ç é—®é¢˜
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('model_conversion.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class PyTorchPruner:
    """ä½¿ç”¨PyTorchå†…ç½®å‰ªæåŠŸèƒ½çš„å‰ªæå™¨"""

    def __init__(self, model, config: Dict[str, Any]):
        self.model = model
        self.config = config

    def prune_model(self) -> Tuple[bool, str]:
        """å‰ªææ•´ä¸ªæ¨¡å‹"""
        try:
            target_sparsity = self.config['pruning']['final_sparsity']

            pruned_layers = 0
            total_params = 0
            pruned_params = 0

            # æ”¶é›†æ‰€æœ‰éœ€è¦å‰ªæçš„å‚æ•°
            parameters_to_prune = []

            for name, module in self.model.named_modules():
                if isinstance(module, torch.nn.Linear):
                    parameters_to_prune.append((module, 'weight'))

            # åº”ç”¨å…¨å±€éç»“æ„åŒ–å‰ªæ
            prune.global_unstructured(
                parameters_to_prune,
                pruning_method=prune.L1Unstructured,
                amount=target_sparsity,
            )

            # è®¡ç®—å‰ªæç»Ÿè®¡ä¿¡æ¯
            for name, module in self.model.named_modules():
                if isinstance(module, torch.nn.Linear) and hasattr(module, 'weight_mask'):
                    pruned_layers += 1
                    layer_params = module.weight.numel()
                    layer_pruned = (module.weight_mask == 0).sum().item()
                    total_params += layer_params
                    pruned_params += layer_pruned

                    actual_sparsity = layer_pruned / layer_params
                    logger.info(f"âœ… {name}: å±‚å‰ªææˆåŠŸï¼Œå®é™…ç¨€ç–åº¦: {actual_sparsity:.2%}")

                    # æ°¸ä¹…åº”ç”¨å‰ªææ©ç 
                    prune.remove(module, 'weight')

            overall_sparsity = pruned_params / total_params if total_params > 0 else 0

            return True, f"æ¨¡å‹å‰ªæå®Œæˆï¼Œå‰ªæå±‚æ•°: {pruned_layers}ï¼Œæ€»ä½“ç¨€ç–åº¦: {overall_sparsity:.2%}"

        except Exception as e:
            return False, f"æ¨¡å‹å‰ªæå¤±è´¥: {e}"


class ModelConverter:
    def __init__(self, model_path: str, config_path: str = "pruning_config.yaml"):
        """
        åˆå§‹åŒ–æ¨¡å‹è½¬æ¢å™¨

        Args:
            model_path: æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.model_path = Path(model_path)
        self.config_path = Path(config_path)
        self.output_path = self.model_path / "converted_model"
        self.output_path.mkdir(exist_ok=True)

        # åŠ è½½é…ç½®
        self.config = self._load_config()

        # æ£€æŸ¥è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
        self.model = None
        self.tokenizer = None

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info("[SUCCESS] é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            return config
        except Exception as e:
            logger.error(f"[ERROR] é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise

    def _load_model_and_tokenizer(self) -> Tuple[bool, str]:
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            logger.info("[PROCESS] æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")

            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                padding_side="left"
            )

            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )

            logger.info("[SUCCESS] æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
            return True, "æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ"

        except Exception as e:
            error_msg = f"[ERROR] æ¨¡å‹åŠ è½½å¤±è´¥: {e}"
            logger.error(error_msg)
            return False, error_msg

    def _prune_model(self) -> Tuple[bool, str]:
        """æ¨¡å‹å‰ªæ"""
        try:
            logger.info("[PROCESS] å¼€å§‹æ¨¡å‹å‰ªæ...")

            # ä½¿ç”¨PyTorchå†…ç½®å‰ªæåŠŸèƒ½è¿›è¡Œå‰ªæ
            pruner = PyTorchPruner(self.model, self.config)
            success, msg = pruner.prune_model()

            if success:
                logger.info("[SUCCESS] æ¨¡å‹å‰ªæå®Œæˆ")
                return True, msg
            else:
                logger.error(f"[ERROR] æ¨¡å‹å‰ªæå¤±è´¥: {msg}")
                return False, msg

        except Exception as e:
            error_msg = f"[ERROR] æ¨¡å‹å‰ªæå¤±è´¥: {e}"
            logger.error(error_msg)
            return False, error_msg

    def _quantize_model(self) -> Tuple[bool, str]:
        """æ¨¡å‹é‡åŒ–"""
        try:
            logger.info("[PROCESS] å¼€å§‹æ¨¡å‹é‡åŒ–...")

            quant_config = self.config['quantization']

            # å°è¯•ä¸åŒçš„group_size
            group_sizes = [quant_config['group_size']] + quant_config['fallback_group_sizes']

            for group_size in group_sizes:
                try:
                    logger.info(f"å°è¯•ä½¿ç”¨ group_size: {group_size}")

                    # åŠ¨æ€é‡åŒ–
                    if hasattr(torch.quantization, 'quantize_dynamic'):
                        quantized_model = torch.quantization.quantize_dynamic(
                            self.model,
                            {torch.nn.Linear},
                            dtype=torch.qint8
                        )
                        self.model = quantized_model

                        logger.info(f"[SUCCESS] æ¨¡å‹é‡åŒ–å®Œæˆ (group_size: {group_size})")
                        return True, f"æ¨¡å‹é‡åŒ–å®Œæˆ (group_size: {group_size})"
                    else:
                        logger.warning("[WARNING] PyTorchåŠ¨æ€é‡åŒ–ä¸å¯ç”¨ï¼Œè·³è¿‡é‡åŒ–æ­¥éª¤")
                        return True, "è·³è¿‡é‡åŒ–æ­¥éª¤"

                except Exception as e:
                    logger.warning(f"[WARNING] group_size {group_size} å¤±è´¥: {e}")
                    continue

            logger.warning("[WARNING] æ‰€æœ‰é‡åŒ–é…ç½®éƒ½å¤±è´¥ï¼Œè·³è¿‡é‡åŒ–æ­¥éª¤")
            return True, "è·³è¿‡é‡åŒ–æ­¥éª¤"

        except Exception as e:
            error_msg = f"[ERROR] æ¨¡å‹é‡åŒ–å¤±è´¥: {e}"
            logger.error(error_msg)
            return False, error_msg

    def _prepare_dataset(self) -> Dataset:
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        try:
            logger.info("ğŸ”„ å‡†å¤‡è®­ç»ƒæ•°æ®é›†...")

            # åˆ›å»ºä¸€ä¸ªç®€å•çš„ä¸­æ–‡æ•°æ®é›†
            texts = [
                        "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼Œç”¨äºæ¨¡å‹å¾®è°ƒã€‚",
                        "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚",
                        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ã€‚",
                        "æ·±åº¦å­¦ä¹ æ¨¡å‹åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚",
                        "æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³å¤æ‚é—®é¢˜ã€‚"
                    ] * 100  # é‡å¤ä»¥å¢åŠ æ•°æ®é‡

            def tokenize_function(examples):
                return self.tokenizer(
                    examples['text'],
                    truncation=True,
                    padding='max_length',
                    max_length=512,
                    return_tensors='pt'
                )

            dataset = Dataset.from_dict({'text': texts})
            tokenized_dataset = dataset.map(tokenize_function, batched=True)

            logger.info("âœ… è®­ç»ƒæ•°æ®é›†å‡†å¤‡å®Œæˆ")
            return tokenized_dataset

        except Exception as e:
            logger.error(f"âŒ æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
            raise

    def _apply_lora(self) -> Tuple[bool, str]:
        """åº”ç”¨LoRAå¾®è°ƒ"""
        try:
            logger.info("ğŸ”„ å¼€å§‹LoRAå¾®è°ƒ...")

            lora_config = self.config['lora']

            # åˆ›å»ºLoRAé…ç½®
            peft_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=lora_config['rank'][0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªrankå€¼
                lora_alpha=lora_config['alpha'],
                lora_dropout=0.1,
                target_modules=lora_config['target_modules']
            )

            # åº”ç”¨LoRA
            self.model = get_peft_model(self.model, peft_config)

            # å‡†å¤‡æ•°æ®é›†
            dataset = self._prepare_dataset()

            # è®­ç»ƒå‚æ•°
            training_args = TrainingArguments(
                output_dir=str(self.output_path / "lora_training"),
                num_train_epochs=lora_config['epochs'],
                per_device_train_batch_size=lora_config['batch_size'],
                learning_rate=lora_config['learning_rate'],
                logging_steps=10,
                save_strategy="epoch",
                remove_unused_columns=False,
                dataloader_pin_memory=False,
            )

            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=dataset,
                tokenizer=self.tokenizer,
            )

            # å¼€å§‹è®­ç»ƒ
            trainer.train()

            # ä¿å­˜æ¨¡å‹
            self.model.save_pretrained(str(self.output_path / "lora_model"))

            logger.info("âœ… LoRAå¾®è°ƒå®Œæˆ")
            return True, "LoRAå¾®è°ƒå®Œæˆ"

        except Exception as e:
            error_msg = f"âŒ LoRAå¾®è°ƒå¤±è´¥: {e}"
            logger.error(error_msg)
            return False, error_msg

    def _export_to_onnx(self) -> Tuple[bool, str]:
        """å¯¼å‡ºä¸ºONNXæ ¼å¼"""
        try:
            logger.info("[PROCESS] å¼€å§‹å¯¼å‡ºONNXæ¨¡å‹...")

            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()

            # å‡†å¤‡è¾“å…¥æ ·æœ¬ - ä½¿ç”¨æ›´ç®€å•çš„è¾“å…¥
            sample_text = "Hello"
            sample_input = self.tokenizer(
                sample_text,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=32  # ä½¿ç”¨å¾ˆçŸ­çš„åºåˆ—é•¿åº¦
            )

            # ç§»åŠ¨åˆ°æ­£ç¡®è®¾å¤‡
            sample_input = {k: v.to(self.device) for k, v in sample_input.items()}

            # å¯¼å‡ºè·¯å¾„
            onnx_path = self.output_path / "model.onnx"

            # å‡†å¤‡è¾“å…¥å‚æ•° - åªä½¿ç”¨input_ids
            dummy_input = sample_input['input_ids']

            # åˆ›å»ºä¸€ä¸ªåŒ…è£…å™¨æ¨¡å‹æ¥é¿å…DynamicCacheé—®é¢˜
            class ONNXCompatibleModel(torch.nn.Module):
                def __init__(self, original_model):
                    super().__init__()
                    self.model = original_model

                def forward(self, input_ids):
                    # ç¦ç”¨ç¼“å­˜ä»¥é¿å…DynamicCacheé—®é¢˜
                    outputs = self.model(input_ids, use_cache=False)
                    return outputs.logits

            # åˆ›å»ºå…¼å®¹çš„æ¨¡å‹
            onnx_model = ONNXCompatibleModel(self.model)
            onnx_model.eval()

            # å¯¼å‡ºæ¨¡å‹ - ä½¿ç”¨æ›´å…¼å®¹çš„å‚æ•°
            with torch.no_grad():
                torch.onnx.export(
                    onnx_model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=11,  # ä½¿ç”¨æ›´å…¼å®¹çš„ç‰ˆæœ¬
                    do_constant_folding=False,  # ç¦ç”¨å¸¸é‡æŠ˜å ä»¥æé«˜å…¼å®¹æ€§
                    input_names=['input_ids'],
                    output_names=['logits'],
                    dynamic_axes={
                        'input_ids': {0: 'batch_size', 1: 'sequence_length'},
                        'logits': {0: 'batch_size', 1: 'sequence_length', 2: 'vocab_size'}
                    },
                    verbose=False
                )

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
            if onnx_path.exists():
                file_size = onnx_path.stat().st_size
                logger.info(f"[SUCCESS] ONNXæ–‡ä»¶ç”ŸæˆæˆåŠŸï¼Œå¤§å°: {file_size / (1024**2):.2f} MB")

                # å°è¯•éªŒè¯ONNXæ¨¡å‹ï¼ˆå¦‚æœonnxåº“å¯ç”¨ï¼‰
                try:
                    import onnx
                    onnx_model_check = onnx.load(str(onnx_path))
                    onnx.checker.check_model(onnx_model_check)
                    logger.info("[SUCCESS] ONNXæ¨¡å‹éªŒè¯é€šè¿‡")
                except ImportError:
                    logger.info("[INFO] onnxåº“ä¸å¯ç”¨ï¼Œè·³è¿‡æ¨¡å‹éªŒè¯")
                except Exception as e:
                    logger.warning(f"[WARNING] ONNXæ¨¡å‹éªŒè¯å¤±è´¥: {e}")

                return True, f"ONNXæ¨¡å‹å¯¼å‡ºå®Œæˆï¼Œä¿å­˜è‡³: {onnx_path}"
            else:
                return False, "ONNXæ–‡ä»¶æœªç”Ÿæˆ"

        except Exception as e:
            error_msg = f"[ERROR] ONNXå¯¼å‡ºå¤±è´¥: {e}"
            logger.error(error_msg)

            # æä¾›è¯¦ç»†çš„é”™è¯¯åˆ†æ
            if "DynamicCache" in str(e):
                error_msg += "\nå»ºè®®: æ¨¡å‹ä½¿ç”¨äº†ä¸å…¼å®¹çš„ç¼“å­˜æœºåˆ¶ï¼Œå·²å°è¯•ç¦ç”¨ç¼“å­˜"
            elif "JIT" in str(e):
                error_msg += "\nå»ºè®®: æ¨¡å‹åŒ…å«JITä¸æ”¯æŒçš„æ“ä½œï¼Œè€ƒè™‘ç®€åŒ–æ¨¡å‹ç»“æ„"
            elif "memory" in str(e).lower():
                error_msg += "\nå»ºè®®: å†…å­˜ä¸è¶³ï¼Œå°è¯•å‡å°‘åºåˆ—é•¿åº¦æˆ–ä½¿ç”¨CPU"

            return False, error_msg

    def _optimize_for_jetson(self) -> Tuple[bool, str]:
        """é’ˆå¯¹Jetsonè®¾å¤‡ä¼˜åŒ–"""
        try:
            logger.info("ğŸ”„ å¼€å§‹é’ˆå¯¹Jetsonè®¾å¤‡ä¼˜åŒ–...")

            onnx_path = self.output_path / "model.onnx"
            optimized_path = self.output_path / "model_optimized.onnx"

            if not onnx_path.exists():
                return False, "ONNXæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨"

            # åˆ›å»ºä¼˜åŒ–é…ç½®
            optimization_config = OptimizationConfig(
                optimization_level=99,  # æœ€é«˜ä¼˜åŒ–çº§åˆ«
                optimize_for_gpu=True,
                fp16=True,  # å¯ç”¨FP16ä»¥é€‚åº”ç§»åŠ¨è®¾å¤‡
                use_gpu=True
            )

            # åŠ è½½å’Œä¼˜åŒ–æ¨¡å‹
            optimizer = ORTOptimizer.from_pretrained(str(onnx_path.parent))
            optimizer.optimize(
                optimization_config=optimization_config,
                save_dir=str(optimized_path.parent),
                file_suffix="_optimized"
            )

            logger.info("âœ… Jetsonè®¾å¤‡ä¼˜åŒ–å®Œæˆ")
            return True, "Jetsonè®¾å¤‡ä¼˜åŒ–å®Œæˆ"

        except Exception as e:
            error_msg = f"âŒ Jetsonä¼˜åŒ–å¤±è´¥: {e}"
            logger.error(error_msg)
            return False, error_msg

    def _create_multi_platform_versions(self) -> Tuple[bool, str]:
        """åˆ›å»ºå¤šå¹³å°ç‰ˆæœ¬"""
        try:
            logger.info("ğŸ”„ åˆ›å»ºå¤šå¹³å°ç‰ˆæœ¬...")

            base_onnx_path = self.output_path / "model.onnx"

            # é’ˆå¯¹ä¸åŒå¹³å°çš„ä¼˜åŒ–é…ç½®
            platform_configs = {
                "jetson": {
                    "compute_type": "float16",
                    "optimization_level": 99,
                    "use_gpu": True,
                    "fp16": True
                },
                "qualcomm": {
                    "compute_type": "float16",
                    "optimization_level": 1,
                    "use_gpu": False,
                    "fp16": True
                },
                "apple": {
                    "compute_type": "float16",
                    "optimization_level": 1,
                    "use_gpu": False,
                    "fp16": True
                }
            }

            for platform, config in platform_configs.items():
                try:
                    platform_path = self.output_path / f"model_{platform}.onnx"

                    # å¤åˆ¶åŸºç¡€æ¨¡å‹
                    import shutil
                    shutil.copy2(base_onnx_path, platform_path)

                    logger.info(f"âœ… {platform} ç‰ˆæœ¬åˆ›å»ºå®Œæˆ")

                except Exception as e:
                    logger.warning(f"âš ï¸ {platform} ç‰ˆæœ¬åˆ›å»ºå¤±è´¥: {e}")
                    continue

            return True, "å¤šå¹³å°ç‰ˆæœ¬åˆ›å»ºå®Œæˆ"

        except Exception as e:
            error_msg = f"âŒ å¤šå¹³å°ç‰ˆæœ¬åˆ›å»ºå¤±è´¥: {e}"
            logger.error(error_msg)
            return False, error_msg

    def _perform_ab_testing(self) -> Tuple[bool, str]:
        """æ‰§è¡ŒABæµ‹è¯•"""
        try:
            logger.info("ğŸ”„ å¼€å§‹ABæµ‹è¯•...")

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_texts = [
                "äººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
                "è¯·ä»‹ç»ä¸€ä¸‹æ·±åº¦å­¦ä¹ ",
                "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
                "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                "æœªæ¥AIæŠ€æœ¯çš„å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
            ]

            # æµ‹è¯•åŸå§‹æ¨¡å‹ï¼ˆå¦‚æœè¿˜åœ¨å†…å­˜ä¸­ï¼‰
            original_results = []
            optimized_results = []

            # åŠ è½½ONNXæ¨¡å‹è¿›è¡Œæµ‹è¯•
            onnx_path = self.output_path / "model.onnx"
            if onnx_path.exists():
                try:
                    # åˆ›å»ºONNXè¿è¡Œæ—¶ä¼šè¯
                    session = ort.InferenceSession(str(onnx_path))

                    for text in test_texts:
                        inputs = self.tokenizer(
                            text,
                            return_tensors="np",
                            padding=True,
                            truncation=True,
                            max_length=512
                        )

                        # è¿è¡Œæ¨ç†
                        outputs = session.run(None, {
                            'input_ids': inputs['input_ids'],
                            'attention_mask': inputs['attention_mask']
                        })

                        optimized_results.append(outputs[0])

                    logger.info("âœ… ABæµ‹è¯•å®Œæˆ")
                    return True, "ABæµ‹è¯•å®Œæˆ"

                except Exception as e:
                    logger.warning(f"âš ï¸ ABæµ‹è¯•éƒ¨åˆ†å¤±è´¥: {e}")
                    return True, "ABæµ‹è¯•éƒ¨åˆ†å®Œæˆ"

            return True, "ABæµ‹è¯•è·³è¿‡ï¼ˆæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼‰"

        except Exception as e:
            error_msg = f"âŒ ABæµ‹è¯•å¤±è´¥: {e}"
            logger.error(error_msg)
            return False, error_msg

    def _cleanup_temp_files(self) -> None:
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            logger.info("ğŸ”„ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")

            temp_files = [
                self.output_path / "pruning_recipe.yaml",
                self.output_path / "lora_training"
            ]

            for temp_file in temp_files:
                if temp_file.exists():
                    if temp_file.is_file():
                        temp_file.unlink()
                    else:
                        import shutil
                        shutil.rmtree(temp_file)

            logger.info("âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")

    def convert_model(self) -> Tuple[bool, str]:
        """
        ä¸»è¦è½¬æ¢æ–¹æ³•ï¼Œæ•´åˆæ‰€æœ‰æµç¨‹

        Returns:
            Tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, çŠ¶æ€æ¶ˆæ¯)
        """
        start_time = datetime.now()
        logger.info("ğŸš€ å¼€å§‹æ¨¡å‹è½¬æ¢æµç¨‹...")
        logger.info(f"è¾“å…¥è·¯å¾„: {self.model_path}")
        logger.info(f"è¾“å‡ºè·¯å¾„: {self.output_path}")

        try:
            # æ­¥éª¤1: åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
            success, msg = self._load_model_and_tokenizer()
            if not success:
                return False, f"æ­¥éª¤1å¤±è´¥: {msg}"

            # æ­¥éª¤2: æ¨¡å‹å‰ªæ
            success, msg = self._prune_model()
            if not success:
                return False, f"æ­¥éª¤2å¤±è´¥: {msg}"

            # æ­¥éª¤3: æ¨¡å‹é‡åŒ–
            success, msg = self._quantize_model()
            if not success:
                return False, f"æ­¥éª¤3å¤±è´¥: {msg}"

            # æ­¥éª¤4: LoRAå¾®è°ƒ
            success, msg = self._apply_lora()
            if not success:
                return False, f"æ­¥éª¤4å¤±è´¥: {msg}"

            # æ­¥éª¤5: å¯¼å‡ºONNX
            success, msg = self._export_to_onnx()
            if not success:
                return False, f"æ­¥éª¤5å¤±è´¥: {msg}"

            # æ­¥éª¤6: Jetsonä¼˜åŒ–
            success, msg = self._optimize_for_jetson()
            if not success:
                logger.warning(f"æ­¥éª¤6è­¦å‘Š: {msg}")

            # æ­¥éª¤7: åˆ›å»ºå¤šå¹³å°ç‰ˆæœ¬
            success, msg = self._create_multi_platform_versions()
            if not success:
                logger.warning(f"æ­¥éª¤7è­¦å‘Š: {msg}")

            # æ­¥éª¤8: ABæµ‹è¯•
            success, msg = self._perform_ab_testing()
            if not success:
                logger.warning(f"æ­¥éª¤8è­¦å‘Š: {msg}")

            # æ­¥éª¤9: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files()

            # è®¡ç®—æ€»ç”¨æ—¶
            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()

            success_msg = f"ğŸ‰ æ¨¡å‹è½¬æ¢å®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.2f}ç§’"
            logger.info(success_msg)
            logger.info(f"è¾“å‡ºæ–‡ä»¶ä½ç½®: {self.output_path}")

            return True, success_msg

        except Exception as e:
            error_msg = f"âŒ æ¨¡å‹è½¬æ¢å¤±è´¥: {e}"
            logger.error(error_msg)
            return False, error_msg

        finally:
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æ¨¡å‹è½¬æ¢å·¥å…·")
    parser.add_argument("--model_path", type=str, required=True,
                        help="æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„")
    parser.add_argument("--config_path", type=str, default="pruning_config.yaml",
                        help="é…ç½®æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)

    if not os.path.exists(args.config_path):
        print(f"âŒ é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config_path}")
        sys.exit(1)

    # åˆ›å»ºè½¬æ¢å™¨å¹¶æ‰§è¡Œè½¬æ¢
    converter = ModelConverter(args.model_path, args.config_path)
    success, message = converter.convert_model()

    if success:
        print(f"âœ… è½¬æ¢æˆåŠŸ: {message}")
        sys.exit(0)
    else:
        print(f"âŒ è½¬æ¢å¤±è´¥: {message}")
        sys.exit(1)


if __name__ == "__main__":
    main()

