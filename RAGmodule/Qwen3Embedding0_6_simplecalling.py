#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-Embedding-0.6B ç®€åŒ–è°ƒç”¨æ¥å£
åŒ…å«æ‰€æœ‰æµ‹è¯•éœ€è¦ç”¨åˆ°çš„æ–¹æ³•ï¼Œæä¾›ç®€å•æ˜“ç”¨çš„API
"""

import os
import sys
import time
import torch
import torch.nn.functional as F
from typing import List, Union, Optional, Tuple, Dict, Any
from transformers import AutoTokenizer, AutoModel
import numpy as np

# å°è¯•å¯¼å…¥sentence_transformersï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨transformers
try:
    from sentence_transformers import SentenceTransformer
    HAS_SENTENCE_TRANSFORMERS = True
except ImportError:
    HAS_SENTENCE_TRANSFORMERS = False
    print("Warning: sentence-transformers not available, using transformers only")


class Qwen3Embedding0_6_SimpleAPI:
    """
    Qwen3-Embedding-0.6B ç®€åŒ–APIç±»
    æä¾›æ‰€æœ‰æµ‹è¯•éœ€è¦çš„æ–¹æ³•ï¼Œæ”¯æŒå¤šç§ä½¿ç”¨æ–¹å¼
    """
    
    def __init__(self, 
                 model_path: str = "./Qwen3-Embedding-0.6B",
                 use_sentence_transformers: bool = None,
                 device: Optional[str] = None,
                 max_length: int = 8192):
        """
        åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            use_sentence_transformers: æ˜¯å¦ä½¿ç”¨sentence_transformersåº“ï¼ŒNoneä¸ºè‡ªåŠ¨é€‰æ‹©
            device: è®¾å¤‡ç±»å‹ ('cpu', 'cuda', 'auto')
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.model_path = model_path
        self.max_length = max_length
        
        # è‡ªåŠ¨é€‰æ‹©ä½¿ç”¨å“ªä¸ªåº“
        if use_sentence_transformers is None:
            self.use_sentence_transformers = HAS_SENTENCE_TRANSFORMERS
        else:
            self.use_sentence_transformers = use_sentence_transformers and HAS_SENTENCE_TRANSFORMERS
        
        # è®¾å¤‡é€‰æ‹©
        if device is None or device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        self.model = None
        self.tokenizer = None
        self.is_loaded = False
        
        # è‡ªåŠ¨åŠ è½½æ¨¡å‹
        self.load_model()
    
    def load_model(self) -> bool:
        """
        åŠ è½½æ¨¡å‹
        
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            if self.use_sentence_transformers:
                print(f"Loading model with sentence_transformers on {self.device}")
                self.model = SentenceTransformer(
                    self.model_path,
                    device=self.device
                )
            else:
                print(f"Loading model with transformers on {self.device}")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path, 
                    padding_side='left'
                )
                self.model = AutoModel.from_pretrained(self.model_path)
                self.model.to(self.device)
                self.model.eval()
            
            self.is_loaded = True
            print("âœ“ Model loaded successfully")
            return True
            
        except Exception as e:
            print(f"âœ— Model loading failed: {str(e)}")
            self.is_loaded = False
            return False
    
    def check_model_loaded(self):
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½"""
        if not self.is_loaded:
            raise RuntimeError("Model not loaded. Please call load_model() first.")
    
    def _last_token_pool(self, last_hidden_states: torch.Tensor, 
                        attention_mask: torch.Tensor) -> torch.Tensor:
        """
        æœ€åä¸€ä¸ªtokenæ± åŒ–ï¼ˆç”¨äºtransformersæ–¹å¼ï¼‰
        """
        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])
        if left_padding:
            return last_hidden_states[:, -1]
        else:
            sequence_lengths = attention_mask.sum(dim=1) - 1
            batch_size = last_hidden_states.shape[0]
            return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]
    
    def encode_text(self, 
                   texts: Union[str, List[str]], 
                   normalize: bool = True,
                   prompt_name: Optional[str] = None) -> np.ndarray:
        """
        ç¼–ç æ–‡æœ¬ä¸ºåµŒå…¥å‘é‡
        
        Args:
            texts: è¾“å…¥æ–‡æœ¬æˆ–æ–‡æœ¬åˆ—è¡¨
            normalize: æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥å‘é‡
            prompt_name: æç¤ºåç§°ï¼ˆä»…ç”¨äºsentence_transformersï¼‰
            
        Returns:
            åµŒå…¥å‘é‡æ•°ç»„
        """
        self.check_model_loaded()
        
        if isinstance(texts, str):
            texts = [texts]
        
        if not texts:
            raise ValueError("Input texts cannot be empty")
        
        try:
            if self.use_sentence_transformers:
                embeddings = self.model.encode(
                    texts, 
                    normalize_embeddings=normalize,
                    prompt_name=prompt_name
                )
                return embeddings
            else:
                # ä½¿ç”¨transformersæ–¹å¼
                batch_dict = self.tokenizer(
                    texts,
                    padding=True,
                    truncation=True,
                    max_length=self.max_length,
                    return_tensors="pt",
                )
                batch_dict = {k: v.to(self.device) for k, v in batch_dict.items()}
                
                with torch.no_grad():
                    outputs = self.model(**batch_dict)
                    embeddings = self._last_token_pool(
                        outputs.last_hidden_state, 
                        batch_dict['attention_mask']
                    )
                    
                    if normalize:
                        embeddings = F.normalize(embeddings, p=2, dim=1)
                    
                    return embeddings.cpu().numpy()
                    
        except Exception as e:
            raise RuntimeError(f"Text encoding failed: {str(e)}")
    
    def calculate_similarity(self, 
                           embeddings1: np.ndarray, 
                           embeddings2: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—åµŒå…¥å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            embeddings1: ç¬¬ä¸€ç»„åµŒå…¥å‘é‡
            embeddings2: ç¬¬äºŒç»„åµŒå…¥å‘é‡
            
        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ
        """
        if self.use_sentence_transformers and hasattr(self.model, 'similarity'):
            return self.model.similarity(embeddings1, embeddings2).numpy()
        else:
            # æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            embeddings1 = torch.tensor(embeddings1)
            embeddings2 = torch.tensor(embeddings2)
            return (embeddings1 @ embeddings2.T).numpy()
    
    def get_embedding_dimension(self) -> int:
        """
        è·å–åµŒå…¥å‘é‡ç»´åº¦
        
        Returns:
            åµŒå…¥å‘é‡ç»´åº¦
        """
        self.check_model_loaded()
        
        if self.use_sentence_transformers:
            return self.model.get_sentence_embedding_dimension()
        else:
            # é€šè¿‡ç¼–ç ä¸€ä¸ªç®€å•æ–‡æœ¬æ¥è·å–ç»´åº¦
            test_embedding = self.encode_text("test")
            return test_embedding.shape[1]
    
    def benchmark_performance(self, 
                            test_text: str = "This is a performance test text", 
                            num_runs: int = 10) -> Dict[str, float]:
        """
        æ€§èƒ½åŸºå‡†æµ‹è¯•
        
        Args:
            test_text: æµ‹è¯•æ–‡æœ¬
            num_runs: è¿è¡Œæ¬¡æ•°
            
        Returns:
            æ€§èƒ½ç»Ÿè®¡å­—å…¸
        """
        self.check_model_loaded()
        
        times = []
        for _ in range(num_runs):
            start_time = time.perf_counter()
            self.encode_text(test_text)
            end_time = time.perf_counter()
            times.append((end_time - start_time) * 1000)
        
        return {
            "avg_time_ms": np.mean(times),
            "std_time_ms": np.std(times),
            "min_time_ms": np.min(times),
            "max_time_ms": np.max(times),
            "num_runs": num_runs
        }
    
    def test_basic_functionality(self) -> Dict[str, Any]:
        """
        æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        results = {
            "model_loaded": False,
            "single_text_encoding": False,
            "batch_text_encoding": False,
            "embedding_dimension": None,
            "similarity_calculation": False,
            "performance": None,
            "errors": []
        }
        
        try:
            # æµ‹è¯•æ¨¡å‹åŠ è½½
            results["model_loaded"] = self.is_loaded
            
            # æµ‹è¯•å•æ–‡æœ¬ç¼–ç 
            try:
                embedding = self.encode_text("Test text")
                results["single_text_encoding"] = True
                results["embedding_dimension"] = embedding.shape[1]
            except Exception as e:
                results["errors"].append(f"Single text encoding failed: {e}")
            
            # æµ‹è¯•æ‰¹é‡ç¼–ç 
            try:
                embeddings = self.encode_text(["Text 1", "Text 2", "Text 3"])
                results["batch_text_encoding"] = embeddings.shape[0] == 3
            except Exception as e:
                results["errors"].append(f"Batch text encoding failed: {e}")
            
            # æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
            try:
                text1_emb = self.encode_text("Hello world")
                text2_emb = self.encode_text("Hello world")
                similarity = self.calculate_similarity(text1_emb, text2_emb)
                results["similarity_calculation"] = similarity[0, 0] > 0.9  # ç›¸åŒæ–‡æœ¬åº”è¯¥é«˜åº¦ç›¸ä¼¼
            except Exception as e:
                results["errors"].append(f"Similarity calculation failed: {e}")
            
            # æµ‹è¯•æ€§èƒ½
            try:
                results["performance"] = self.benchmark_performance(num_runs=3)
            except Exception as e:
                results["errors"].append(f"Performance test failed: {e}")
                
        except Exception as e:
            results["errors"].append(f"General test failed: {e}")
        
        return results
    
    def test_edge_cases(self) -> Dict[str, Any]:
        """
        æµ‹è¯•è¾¹ç•Œæ¡ä»¶
        
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        results = {
            "empty_input_handled": False,
            "short_text_handled": False,
            "long_text_handled": False,
            "errors": []
        }
        
        try:
            # æµ‹è¯•ç©ºè¾“å…¥
            try:
                self.encode_text([])
                results["errors"].append("Empty input should raise exception")
            except ValueError:
                results["empty_input_handled"] = True
            except Exception as e:
                results["errors"].append(f"Unexpected error for empty input: {e}")
            
            # æµ‹è¯•æçŸ­æ–‡æœ¬
            try:
                short_embedding = self.encode_text("a")
                results["short_text_handled"] = short_embedding.shape[1] > 0
            except Exception as e:
                results["errors"].append(f"Short text handling failed: {e}")
            
            # æµ‹è¯•é•¿æ–‡æœ¬
            try:
                long_text = "This is a very long text. " * 1000
                long_embedding = self.encode_text(long_text)
                results["long_text_handled"] = long_embedding.shape[1] > 0
            except Exception as e:
                results["errors"].append(f"Long text handling failed: {e}")
                
        except Exception as e:
            results["errors"].append(f"Edge case test failed: {e}")
        
        return results
    
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """
        è¿è¡Œç»¼åˆæµ‹è¯•
        
        Returns:
            å®Œæ•´æµ‹è¯•ç»“æœ
        """
        print("=" * 60)
        print("Qwen3-Embedding-0.6B Comprehensive Test")
        print("=" * 60)
        
        # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
        print("\n1. Testing basic functionality...")
        basic_results = self.test_basic_functionality()
        
        # è¾¹ç•Œæ¡ä»¶æµ‹è¯•
        print("2. Testing edge cases...")
        edge_results = self.test_edge_cases()
        
        # æ±‡æ€»ç»“æœ
        all_results = {
            "basic_functionality": basic_results,
            "edge_cases": edge_results,
            "summary": {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "pass_rate": 0.0
            }
        }
        
        # è®¡ç®—é€šè¿‡ç‡
        tests = [
            basic_results["model_loaded"],
            basic_results["single_text_encoding"],
            basic_results["batch_text_encoding"],
            basic_results["similarity_calculation"],
            edge_results["empty_input_handled"],
            edge_results["short_text_handled"],
            edge_results["long_text_handled"]
        ]
        
        total_tests = len(tests)
        passed_tests = sum(tests)
        pass_rate = (passed_tests / total_tests) * 100
        
        all_results["summary"]["total_tests"] = total_tests
        all_results["summary"]["passed_tests"] = passed_tests
        all_results["summary"]["failed_tests"] = total_tests - passed_tests
        all_results["summary"]["pass_rate"] = pass_rate
        
        # æ‰“å°ç»“æœ
        print(f"\n3. Test Summary:")
        print(f"   Total tests: {total_tests}")
        print(f"   Passed: {passed_tests}")
        print(f"   Failed: {total_tests - passed_tests}")
        print(f"   Pass rate: {pass_rate:.1f}%")
        
        if pass_rate >= 90:
            print("   Status: ğŸ‰ Excellent!")
        elif pass_rate >= 70:
            print("   Status: âœ… Good!")
        elif pass_rate >= 50:
            print("   Status: âš ï¸ Fair")
        else:
            print("   Status: âŒ Poor")
        
        print("=" * 60)
        
        return all_results
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        info = {
            "model_path": self.model_path,
            "device": self.device,
            "max_length": self.max_length,
            "use_sentence_transformers": self.use_sentence_transformers,
            "is_loaded": self.is_loaded,
            "embedding_dimension": None,
            "library_used": "sentence_transformers" if self.use_sentence_transformers else "transformers"
        }
        
        if self.is_loaded:
            try:
                info["embedding_dimension"] = self.get_embedding_dimension()
            except:
                info["embedding_dimension"] = "Unknown"
        
        return info


# ä¾¿æ·å‡½æ•°
def quick_test(model_path: str = "./Qwen3-Embedding-0.6B") -> Dict[str, Any]:
    """
    å¿«é€Ÿæµ‹è¯•å‡½æ•°
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        
    Returns:
        æµ‹è¯•ç»“æœ
    """
    api = Qwen3Embedding0_6_SimpleAPI(model_path=model_path)
    return api.run_comprehensive_test()


def quick_encode(texts: Union[str, List[str]], 
                model_path: str = "./Qwen3-Embedding-0.6B") -> np.ndarray:
    """
    å¿«é€Ÿç¼–ç å‡½æ•°
    
    Args:
        texts: è¦ç¼–ç çš„æ–‡æœ¬
        model_path: æ¨¡å‹è·¯å¾„
        
    Returns:
        åµŒå…¥å‘é‡
    """
    api = Qwen3Embedding0_6_SimpleAPI(model_path=model_path)
    return api.encode_text(texts)


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•ç¤ºä¾‹
    print("Running Qwen3-Embedding-0.6B Simple API Test...")
    results = quick_test()
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    api = Qwen3Embedding0_6_SimpleAPI()
    info = api.get_model_info()
    print(f"\nModel Info: {info}")
