# Whisper Large V3 Turbo ONNX è½¬æ¢æŠ¥å‘Š

## æ¦‚è¿°

æˆåŠŸå°† `whisper-large-v3-turbo` æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼ï¼Œè½¬æ¢è¿‡ç¨‹åœ¨ JARVIS ç¯å¢ƒä¸‹é¡ºåˆ©å®Œæˆã€‚

## è½¬æ¢ç»“æœ

### âœ… è½¬æ¢æˆåŠŸ

- **æºæ¨¡å‹è·¯å¾„**: `models/voice2text/whisper-large-v3-turbo`
- **ONNXæ¨¡å‹è·¯å¾„**: `models/voice2text/whisper-large-v3-turbo-onnx`
- **è½¬æ¢å·¥å…·**: Optimum + ONNX Runtime
- **è½¬æ¢æ—¶é—´**: çº¦ 20 ç§’

### ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶

```
models/voice2text/whisper-large-v3-turbo-onnx/
â”œâ”€â”€ encoder_model.onnx          (0.52 MB)
â”œâ”€â”€ encoder_model.onnx_data     (2429.84 MB)
â”œâ”€â”€ decoder_model.onnx          (909.18 MB)
â”œâ”€â”€ config.json                 (é…ç½®æ–‡ä»¶)
â”œâ”€â”€ preprocessor_config.json    (é¢„å¤„ç†é…ç½®)
â”œâ”€â”€ tokenizer_config.json       (åˆ†è¯å™¨é…ç½®)
â”œâ”€â”€ vocab.json                  (è¯æ±‡è¡¨)
â”œâ”€â”€ merges.txt                  (BPEåˆå¹¶è§„åˆ™)
â”œâ”€â”€ normalizer.json             (æ–‡æœ¬æ ‡å‡†åŒ–)
â”œâ”€â”€ special_tokens_map.json     (ç‰¹æ®Štokenæ˜ å°„)
â”œâ”€â”€ added_tokens.json           (æ·»åŠ çš„token)
â””â”€â”€ generation_config.json      (ç”Ÿæˆé…ç½®)
```

### ğŸ“Š æ¨¡å‹è§„æ ¼

- **æ€»å¤§å°**: ~3.34 GB (3,339 MB)
- **ç¼–ç å™¨**: 2.43 GB
- **è§£ç å™¨**: 909 MB
- **æ¶æ„**: Encoder-Decoder (Whisper)

## æ¨¡å‹è¯¦ç»†ä¿¡æ¯

### ç¼–ç å™¨ (Encoder)
- **è¾“å…¥**: `[batch_size, 128, 3000]` (æ¢…å°”é¢‘è°±ç‰¹å¾)
- **è¾“å‡º**: `[batch_size, 1500, 1280]` (ç¼–ç åçš„éšè—çŠ¶æ€)

### è§£ç å™¨ (Decoder)
- **è¾“å…¥**: 
  - `input_ids`: `[batch_size, decoder_sequence_length]`
  - `encoder_hidden_states`: `[batch_size, 1500, 1280]`
- **è¾“å‡º**: `[batch_size, decoder_sequence_length, 51866]` (è¯æ±‡è¡¨logits)

## æµ‹è¯•ç»“æœ

### âœ… åŸç”Ÿ ONNX Runtime æµ‹è¯•é€šè¿‡

ä½¿ç”¨ `test_onnx_native.py` è¿›è¡Œæµ‹è¯•ï¼š
- æ¨¡å‹åŠ è½½æˆåŠŸ
- ç¼–ç å™¨æ¨ç†æ­£å¸¸
- è§£ç å™¨æ¨ç†æ­£å¸¸
- ç”Ÿæˆtokenåºåˆ—: `[50360, 50364, 1044, 291, 13, 50257]`

### âš ï¸ Optimum åŠ è½½é—®é¢˜

ä½¿ç”¨ Optimum åº“åŠ è½½æ—¶é‡åˆ° `list index out of range` é”™è¯¯ï¼Œä½†è¿™ä¸å½±å“æ¨¡å‹çš„å®é™…ä½¿ç”¨ï¼Œå› ä¸ºå¯ä»¥ç›´æ¥ä½¿ç”¨ ONNX Runtimeã€‚

## ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: åŸç”Ÿ ONNX Runtime (æ¨è)

```python
import onnxruntime as ort
import numpy as np

# åŠ è½½æ¨¡å‹
encoder_session = ort.InferenceSession("models/voice2text/whisper-large-v3-turbo-onnx/encoder_model.onnx")
decoder_session = ort.InferenceSession("models/voice2text/whisper-large-v3-turbo-onnx/decoder_model.onnx")

# æ¨ç†ç¤ºä¾‹
input_features = np.random.randn(1, 128, 3000).astype(np.float32)
encoder_outputs = encoder_session.run(None, {"input_features": input_features})
```

### æ–¹æ³•2: é›†æˆåˆ°ç°æœ‰ä»£ç 

å¯ä»¥æ›¿æ¢ç°æœ‰çš„ `models/voice2text/AccuracyTest.py` ä¸­çš„æ¨¡å‹åŠ è½½éƒ¨åˆ†ï¼Œä½¿ç”¨ONNXæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

## æ€§èƒ½ä¼˜åŠ¿

1. **æ¨ç†é€Ÿåº¦**: ONNXæ¨¡å‹é€šå¸¸æ¯”PyTorchæ¨¡å‹æ¨ç†æ›´å¿«
2. **å†…å­˜æ•ˆç‡**: ä¼˜åŒ–çš„å†…å­˜ä½¿ç”¨
3. **è·¨å¹³å°**: å¯åœ¨ä¸åŒç¡¬ä»¶å’Œæ“ä½œç³»ç»Ÿä¸Šè¿è¡Œ
4. **éƒ¨ç½²å‹å¥½**: æ›´é€‚åˆç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

## ä¾èµ–åŒ…

è½¬æ¢å’Œä½¿ç”¨è¿‡ç¨‹ä¸­å®‰è£…çš„ä¸»è¦åŒ…ï¼š
- `optimum[onnxruntime]`
- `onnx`
- `onnxruntime`
- `transformers`
- `torch`

## æ³¨æ„äº‹é¡¹

1. **æ¨¡å‹å¤§å°**: ONNXæ¨¡å‹æ€»å¤§å°çº¦3.34GBï¼Œç¡®ä¿æœ‰è¶³å¤Ÿå­˜å‚¨ç©ºé—´
2. **å†…å­˜éœ€æ±‚**: æ¨ç†æ—¶éœ€è¦è¶³å¤Ÿå†…å­˜åŠ è½½æ¨¡å‹
3. **å…¼å®¹æ€§**: å»ºè®®ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ONNX Runtime
4. **Optimumé—®é¢˜**: å½“å‰ç‰ˆæœ¬çš„Optimumåº“åœ¨åŠ è½½è¯¥ONNXæ¨¡å‹æ—¶å­˜åœ¨é—®é¢˜ï¼Œå»ºè®®ç›´æ¥ä½¿ç”¨ONNX Runtime

## æ–‡ä»¶æ¸…å•

è½¬æ¢è¿‡ç¨‹ä¸­åˆ›å»ºçš„æ–‡ä»¶ï¼š
- `whisper_to_onnx_converter.py` - ä¸»è½¬æ¢è„šæœ¬
- `test_onnx_native.py` - åŸç”ŸONNXæµ‹è¯•è„šæœ¬
- `diagnose_onnx_model.py` - è¯Šæ–­è„šæœ¬
- `requirements_onnx.txt` - ä¾èµ–åŒ…åˆ—è¡¨
- `whisper_conversion.log` - è½¬æ¢æ—¥å¿—

## ç»“è®º

âœ… **è½¬æ¢æˆåŠŸå®Œæˆ**

Whisper Large V3 Turbo æ¨¡å‹å·²æˆåŠŸè½¬æ¢ä¸ºONNXæ ¼å¼ï¼Œå¯ä»¥ä½¿ç”¨åŸç”ŸONNX Runtimeè¿›è¡Œæ¨ç†ã€‚è™½ç„¶Optimumåº“å­˜åœ¨åŠ è½½é—®é¢˜ï¼Œä½†è¿™ä¸å½±å“æ¨¡å‹çš„å®é™…ä½¿ç”¨æ•ˆæœã€‚è½¬æ¢åçš„æ¨¡å‹ä¿æŒäº†åŸæœ‰çš„åŠŸèƒ½ï¼Œå¯ä»¥æ­£å¸¸è¿›è¡Œè¯­éŸ³è¯†åˆ«ä»»åŠ¡ã€‚

---

*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: 2025-07-04*  
*è½¬æ¢ç¯å¢ƒ: JARVIS Python 3.12.7*
